# evaluate.py

import torch
import torch.nn as nn
import logging
import argparse
import sys
import numpy as np
from sklearn.metrics import roc_auc_score, average_precision_score
from tqdm import tqdm

# ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ù…Ø§Ú˜ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡
from utils import set_seed
from mamba_dataset import get_mamba_dataloader
from models import MambaAnomalyDetector

logger = logging.getLogger("EVAL")
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

def calculate_anomaly_scores(model, dataloader, device):
    """
    Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø®Ø·Ø§ÛŒ Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ (Anomaly Score) Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú¯Ø±Ù‡ Ø¯Ø± DataLoader.
    """
    model.eval() # Ù…Ø¯Ù„ Ø±Ø§ Ø¯Ø± Ø­Ø§Ù„Øª Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù‚Ø±Ø§Ø± Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ…
    
    # Ù„ÛŒØ³Øªâ€ŒÙ‡Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
    all_scores = [] # Ø§Ù…ØªÛŒØ§Ø² Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒ (Ø®Ø·Ø§ÛŒ Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ)
    all_labels = [] # Ù„ÛŒØ¨Ù„â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ (0: Ù†Ø±Ù…Ø§Ù„ØŒ 1: Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒ)

    # ğŸ’¡ ØªØ§Ø¨Ø¹ Ø²ÛŒØ§Ù†: MSE Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø®Ø·Ø§ÛŒ Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ
    reconstruction_loss_fn = nn.MSELoss(reduction='none') # reduction='none' Ø¨Ø±Ø§ÛŒ Ú¯Ø±ÙØªÙ† Ø®Ø·Ø§ÛŒ Ù‡Ø± Ù†Ù…ÙˆÙ†Ù‡

    with torch.no_grad(): # Ø¯Ø± Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒØŒ Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú¯Ø±Ø§Ø¯ÛŒØ§Ù† Ù†ÛŒØ³Øª
        for masked_seq, target_feat, anomaly_label in tqdm(dataloader, desc="Calculating Anomaly Scores"):
            
            # Ø§Ù†ØªÙ‚Ø§Ù„ Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡ Ø¯Ø³ØªÚ¯Ø§Ù‡ (GPU)
            masked_seq = masked_seq.to(device)
            target_feat = target_feat.to(device)
            
            # Forward Pass: Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ Ù‡Ø¯Ù
            reconstructed_feat = model(masked_seq)
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø®Ø·Ø§ÛŒ Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ (MSE Loss) Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù†Ù…ÙˆÙ†Ù‡
            # Ø®Ø±ÙˆØ¬ÛŒ: [Batch_Size, Feature_Dim]
            reconstruction_error_per_feature = reconstruction_loss_fn(reconstructed_feat, target_feat)
            
            # Ø¬Ù…Ø¹ Ø®Ø·Ø§Ù‡Ø§ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ú¯Ø±ÙØªÙ† ÛŒÚ© "Ø§Ù…ØªÛŒØ§Ø²" Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù†ÙˆØ¯ Ø¯Ø± Batch
            # Ø®Ø±ÙˆØ¬ÛŒ: [Batch_Size]
            node_anomaly_score = reconstruction_error_per_feature.mean(dim=1)
            
            # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
            all_scores.append(node_anomaly_score.cpu().numpy())
            all_labels.append(anomaly_label.cpu().numpy())

    # ØªØ±Ú©ÛŒØ¨ ØªÙ…Ø§Ù… Ø¨Ú†â€ŒÙ‡Ø§
    scores = np.concatenate(all_scores)
    labels = np.concatenate(all_labels)
    
    return scores, labels


def evaluate_model(args):
    """
    Ø§Ø¬Ø±Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ Ùˆ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø¯Ù‚Øª.
    """
    
    logger.info("--- Starting Model Evaluation ---")
    
    # 1. ØªÙ†Ø¸ÛŒÙ… Ø¯Ø³ØªÚ¯Ø§Ù‡ Ùˆ Seed
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"Using Device: {device}")
    set_seed(args.seed)
    
    # 2. Ù„ÙˆØ¯ Ø¯Ø§Ø¯Ù‡ Ùˆ DataLoader (Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªÙ…Ø§Ù… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ)
    dataloader = get_mamba_dataloader(
        walk_length=args.walk_length, 
        batch_size=args.batch_size, 
        data_dir=args.data_dir,
        shuffle=False # Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ø¨Ø§ÛŒØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ù‡ Ù‡Ù… Ø¨Ø²Ù†ÛŒÙ…
    )
    
    # 3. ØªØ¹Ø±ÛŒÙ Ùˆ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯Ù‡
    first_batch = next(iter(dataloader))
    feat_dim = first_batch[1].shape[1] 
    
    model = MambaAnomalyDetector(
        input_dim=feat_dim, 
        d_model=args.d_model, 
        n_layer=args.n_layer
    ).to(device)
    
    try:
        model.load_state_dict(torch.load(args.model_path, map_location=device))
        logger.info(f"Model weights successfully loaded from: {args.model_path}")
    except FileNotFoundError:
        logger.error(f"FATAL: Model file not found at {args.model_path}. Please run train.py first.")
        sys.exit(1)
        
    # 4. Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø²Ø§Øª Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒ
    scores, labels = calculate_anomaly_scores(model, dataloader, device)
    
    # 5. Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ (Metrics)
    
    # 5.1. AUC-ROC (Area Under the Receiver Operating Characteristic Curve)
    # Ø§ÛŒÙ† Ù…Ø¹ÛŒØ§Ø± Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ù…Ø¯Ù„ Ú†Ù‚Ø¯Ø± ØªÙˆØ§Ù†Ø§ÛŒÛŒ ØªÙÚ©ÛŒÚ© Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒâ€ŒÙ‡Ø§ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ø±Ù…Ø§Ù„ Ø±Ø§ Ø¯Ø§Ø±Ø¯.
    auc_roc = roc_auc_score(labels, scores)
    
    # 5.2. AUC-PR (Area Under the Precision-Recall Curve)
    # Ø¯Ø± Ù…Ø³Ø§Ø¦Ù„ Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒ Ú©Ù‡ Ù„ÛŒØ¨Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø«Ø¨Øª (Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒâ€ŒÙ‡Ø§) Ø¨Ø³ÛŒØ§Ø± Ú©Ù… Ù‡Ø³ØªÙ†Ø¯ØŒ AUC-PR Ù…Ø¹ÛŒØ§Ø± Ù…Ù‡Ù…â€ŒØªØ±ÛŒ Ø§Ø³Øª.
    auc_pr = average_precision_score(labels, scores)
    
    # 6. Ù†Ù…Ø§ÛŒØ´ Ù†ØªØ§ÛŒØ¬
    logger.info("--- Evaluation Results ---")
    logger.info(f"Total Nodes Evaluated: {len(labels)}")
    logger.info(f"True Anomaly Nodes: {np.sum(labels)}")
    logger.info(f"AUC-ROC Score: {auc_roc:.4f}")
    logger.info(f"AUC-PR Score: {auc_pr:.4f}")
    logger.info("--------------------------")
    
# #####################################################################

def arg_parse():
    """ØªØ¹Ø±ÛŒÙ Ùˆ Ù¾Ø§Ø±Ø³ Ú©Ø±Ø¯Ù† Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ø®Ø· ÙØ±Ù…Ø§Ù† Ù¾Ø±ÙˆÚ˜Ù‡."""
    parser = argparse.ArgumentParser(description="Mamba Anomaly Detector Evaluation.")
    
    # --- Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…Ø¯Ù„ Ùˆ Ù…Ø³ÛŒØ± ---
    parser.add_argument('--model_path', type=str, default='best_mamba_model.pt',
                        help='Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ ÙˆØ²Ù† Ù…Ø¯Ù„ (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: best_mamba_model.pt)')
    parser.add_argument('--d_model', type=int, default=128, 
                        help='Ø¨Ø¹Ø¯ Ø¯Ø§Ø®Ù„ÛŒ (Embedding) Ù…Ø¯Ù„ Mamba (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: 128)')
    parser.add_argument('--n_layer', type=int, default=4, 
                        help='ØªØ¹Ø¯Ø§Ø¯ Ø¨Ù„ÙˆÚ©â€ŒÙ‡Ø§ÛŒ Mamba (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: 4)')

    # --- Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø¯Ø§Ø¯Ù‡ ---
    parser.add_argument('--data_dir', type=str, default='data/raw',
                        help='Ù…Ø³ÛŒØ± Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒ Ø­Ø§ÙˆÛŒ ÙØ§ÛŒÙ„ YelpChi.mat (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: data/raw)')
    parser.add_argument('--walk_length', type=int, default=32,
                        help='Ø·ÙˆÙ„ Ù…Ø³ÛŒØ± ØªØµØ§Ø¯ÙÛŒ (Sequence Length) Ø¨Ø±Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ Mamba (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: 32)')
    parser.add_argument('--batch_size', type=int, default=512, # Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø¨Ú† Ø¨Ø²Ø±Ú¯ØªØ± Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
                        help='Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø¨Ú† (Batch Size) Ø¨Ø±Ø§ÛŒ DataLoader (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: 512)')
    parser.add_argument('--seed', type=int, default=42,
                        help='Seed Ø¨Ø±Ø§ÛŒ ØªØ¶Ù…ÛŒÙ† ØªÚ©Ø±Ø§Ø±Ù¾Ø°ÛŒØ±ÛŒ Ù†ØªØ§ÛŒØ¬ (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: 42)')

    return parser.parse_args()


if __name__ == "__main__":
    args = arg_parse()
    # âš ï¸ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø­Ø§ØµÙ„ Ú©Ù†ÛŒØ¯ Ú©Ù‡ Ù¾Ú©ÛŒØ¬ scikit-learn Ù†ØµØ¨ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯
    # !conda run -n old_env pip install scikit-learn
    
    # Ù…Ø§ Ø§Ø² tqdm.write Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ù…ÛŒÚ©Ù†ÛŒÙ… Ø²ÛŒØ±Ø§ Ø¯Ø± evaluateØŒ Ù„Ø§Ú¯â€ŒÙ‡Ø§ Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ ØªØ¯Ø§Ø®Ù„ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯
    evaluate_model(args)
